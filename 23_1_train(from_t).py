# train_lightgbm_optuna.py

import pandas as pd
import numpy as np
import joblib
import optuna
import json

from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import roc_auc_score, accuracy_score
import lightgbm as lgb
from imblearn.over_sampling import SMOTE

import warnings
warnings.filterwarnings("ignore")

###################################################
# 0) ê³µí†µ ì „ì²˜ë¦¬ì— ì“°ì¼ í•¨ìˆ˜ë“¤
###################################################
# ë‚˜ì´ ë³€í™˜ (ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´)
age_map_surgery = {
    "ë§Œ18-34ì„¸": 26,
    "ë§Œ35-37ì„¸": 36,
    "ë§Œ38-39ì„¸": 38,
    "ë§Œ38-40ì„¸": 39,
    "ë§Œ40-42ì„¸": 41,
    "ë§Œ43-44ì„¸": 43,
    "ë§Œ45-50ì„¸": 47,
    "ì•Œ_ìˆ˜_ì—†ìŒ": np.nan
}

def parse_age_surgery(x):
    return age_map_surgery.get(str(x).strip(), np.nan)

# "0íšŒ","1íšŒ" ~ "6íšŒ ì´ìƒ" í˜•íƒœ íšŸìˆ˜ë¥¼ ìˆ«ìë¡œ ë³€í™˜
def parse_cycle_count(x):
    if isinstance(x, str):
        if "ì´ìƒ" in x:
            return 6
        elif "íšŒ" in x:
            return pd.to_numeric(x.replace("íšŒ", ""), errors="coerce")
    return pd.to_numeric(x, errors="coerce")

# True/False, 0/1, 'False'/'True' ê°™ì€ ë¶ˆë¦¬ì–¸ ì»¬ëŸ¼ í†µì¼
def parse_bool(x):
    if pd.isna(x):
        return -1
    if isinstance(x, str):
        lx = x.strip().lower()
        if lx in ["true", "1"]:
            return 1
        elif lx in ["false", "0"]:
            return 0
        else:
            return -1
    val = pd.to_numeric(x, errors="coerce")
    if pd.isna(val):
        return -1
    return 1 if val == 1 else 0

###################################################
# 1) í•™ìŠµ(Training) ì‹¤í–‰ ë¶€ë¶„
###################################################
if __name__ == "__main__":
    # (A) ë°ì´í„° ë¡œë“œ
    train = pd.read_csv("./0_rawdataset/train.csv")

    # ê³µë°± ì»¬ëŸ¼ëª… â†’ ë°‘ì¤„ ì¹˜í™˜ (ì˜ˆ: "ì‹œìˆ  ì‹œê¸° ì½”ë“œ" â†’ "ì‹œìˆ _ì‹œê¸°_ì½”ë“œ")
    train.columns = [c.replace(" ", "_") for c in train.columns]

    # ID ì œê±° (ìˆë‹¤ë©´)
    if "ID" in train.columns:
        train.drop(columns=["ID"], inplace=True)

    # íƒ€ê²Ÿ(ì„ì‹ _ì„±ê³µ_ì—¬ë¶€) ì§€ì •
    target_col = "ì„ì‹ _ì„±ê³µ_ì—¬ë¶€"
    if train[target_col].dtype == object:
        train[target_col] = train[target_col].apply(lambda v: 1 if str(v).strip() == "1" else 0)

    # ë¼ë²¨ ì¸ì½”ë”ë“¤ì„ ì €ì¥í•  dict
    label_encoders = {}

    ############################################
    # (B) ì „ì²˜ë¦¬
    ############################################

    # 1) ì‹œìˆ _ì‹œê¸°_ì½”ë“œ (ë²”ì£¼í˜•)
    code_col = "ì‹œìˆ _ì‹œê¸°_ì½”ë“œ"
    if code_col in train.columns:
        train[code_col].fillna("unknown", inplace=True)
        unique_vals = train[code_col].astype(str).unique().tolist()
        if "unknown" not in unique_vals:
            unique_vals.append("unknown")
        le_code = LabelEncoder()
        le_code.fit(unique_vals)
        train[code_col] = le_code.transform(train[code_col].astype(str))
        label_encoders[code_col] = le_code

    # 2) ì‹œìˆ _ë‹¹ì‹œ_ë‚˜ì´ (ìˆ«ì ë³€í™˜)
    if "ì‹œìˆ _ë‹¹ì‹œ_ë‚˜ì´" in train.columns:
        train["ì‹œìˆ _ë‹¹ì‹œ_ë‚˜ì´"] = train["ì‹œìˆ _ë‹¹ì‹œ_ë‚˜ì´"].apply(parse_age_surgery)

    # 3) ì„ì‹ _ì‹œë„_ë˜ëŠ”_ë§ˆì§€ë§‰_ì„ì‹ _ê²½ê³¼_ì—°ìˆ˜ (ìˆ«ì ë³€í™˜)
    gap_col = "ì„ì‹ _ì‹œë„_ë˜ëŠ”_ë§ˆì§€ë§‰_ì„ì‹ _ê²½ê³¼_ì—°ìˆ˜"
    if gap_col in train.columns:
        train[gap_col] = pd.to_numeric(train[gap_col], errors="coerce")

    # 4) ë²”ì£¼í˜• & ë¶ˆë¦¬ì–¸
    #    ë°°ë€_ìê·¹_ì—¬ë¶€, ë°°ë€_ìœ ë„_ìœ í˜•, ë‹¨ì¼_ë°°ì•„_ì´ì‹_ì—¬ë¶€, ì°©ìƒ_ì „_ìœ ì „_ê²€ì‚¬_ì‚¬ìš©_ì—¬ë¶€ ë“±
    cat_cols = []
    bool_cols = []

    # ì‹œìˆ _ìœ í˜•, íŠ¹ì •_ì‹œìˆ _ìœ í˜•, ë°°ë€_ìœ ë„_ìœ í˜• â†’ ë²”ì£¼í˜•
    if "ì‹œìˆ _ìœ í˜•" in train.columns:
        cat_cols.append("ì‹œìˆ _ìœ í˜•")
    if "íŠ¹ì •_ì‹œìˆ _ìœ í˜•" in train.columns:
        cat_cols.append("íŠ¹ì •_ì‹œìˆ _ìœ í˜•")
    if "ë°°ë€_ìœ ë„_ìœ í˜•" in train.columns:
        cat_cols.append("ë°°ë€_ìœ ë„_ìœ í˜•")

    # ë°°ë€_ìê·¹_ì—¬ë¶€ â†’ boolì´ë¼ê³  ê°€ì •
    if "ë°°ë€_ìê·¹_ì—¬ë¶€" in train.columns:
        bool_cols.append("ë°°ë€_ìê·¹_ì—¬ë¶€")

    # ë‹¨ì¼_ë°°ì•„_ì´ì‹_ì—¬ë¶€, ì°©ìƒ_ì „_ìœ ì „_ê²€ì‚¬_ì‚¬ìš©_ì—¬ë¶€, ì°©ìƒ_ì „_ìœ ì „_ì§„ë‹¨_ì‚¬ìš©_ì—¬ë¶€
    if "ë‹¨ì¼_ë°°ì•„_ì´ì‹_ì—¬ë¶€" in train.columns:
        bool_cols.append("ë‹¨ì¼_ë°°ì•„_ì´ì‹_ì—¬ë¶€")
    if "ì°©ìƒ_ì „_ìœ ì „_ê²€ì‚¬_ì‚¬ìš©_ì—¬ë¶€" in train.columns:
        bool_cols.append("ì°©ìƒ_ì „_ìœ ì „_ê²€ì‚¬_ì‚¬ìš©_ì—¬ë¶€")
    if "ì°©ìƒ_ì „_ìœ ì „_ì§„ë‹¨_ì‚¬ìš©_ì—¬ë¶€" in train.columns:
        bool_cols.append("ì°©ìƒ_ì „_ìœ ì „_ì§„ë‹¨_ì‚¬ìš©_ì—¬ë¶€")

    # ê¸°ì¦_ë°°ì•„_ì‚¬ìš©_ì—¬ë¶€, ëŒ€ë¦¬ëª¨_ì—¬ë¶€, PGD_ì‹œìˆ _ì—¬ë¶€, PGS_ì‹œìˆ _ì—¬ë¶€
    if "ê¸°ì¦_ë°°ì•„_ì‚¬ìš©_ì—¬ë¶€" in train.columns:
        bool_cols.append("ê¸°ì¦_ë°°ì•„_ì‚¬ìš©_ì—¬ë¶€")
    if "ëŒ€ë¦¬ëª¨_ì—¬ë¶€" in train.columns:
        bool_cols.append("ëŒ€ë¦¬ëª¨_ì—¬ë¶€")
    if "PGD_ì‹œìˆ _ì—¬ë¶€" in train.columns:
        bool_cols.append("PGD_ì‹œìˆ _ì—¬ë¶€")
    if "PGS_ì‹œìˆ _ì—¬ë¶€" in train.columns:
        bool_cols.append("PGS_ì‹œìˆ _ì—¬ë¶€")

    # 5) ë¶ˆì„ ì›ì¸ ê´€ë ¨ ë³€ìˆ˜ (ë‚¨ì„±, ì—¬ì„±, ë¶€ë¶€, ë¶ˆëª…í™• ë“±)ë„ bool ì²˜ë¦¬
    for c in [
        "ë‚¨ì„±_ì£¼_ë¶ˆì„_ì›ì¸","ë‚¨ì„±_ë¶€_ë¶ˆì„_ì›ì¸","ì—¬ì„±_ì£¼_ë¶ˆì„_ì›ì¸","ì—¬ì„±_ë¶€_ë¶ˆì„_ì›ì¸",
        "ë¶€ë¶€_ì£¼_ë¶ˆì„_ì›ì¸","ë¶€ë¶€_ë¶€_ë¶ˆì„_ì›ì¸","ë¶ˆëª…í™•_ë¶ˆì„_ì›ì¸","ë¶ˆì„_ì›ì¸_-_ë‚œê´€_ì§ˆí™˜",
        "ë¶ˆì„_ì›ì¸_-_ë‚¨ì„±_ìš”ì¸","ë¶ˆì„_ì›ì¸_-_ë°°ë€_ì¥ì• ","ë¶ˆì„_ì›ì¸_-_ì—¬ì„±_ìš”ì¸",
        "ë¶ˆì„_ì›ì¸_-_ìê¶ê²½ë¶€_ë¬¸ì œ","ë¶ˆì„_ì›ì¸_-_ìê¶ë‚´ë§‰ì¦","ë¶ˆì„_ì›ì¸_-_ì •ì_ë†ë„",
        "ë¶ˆì„_ì›ì¸_-_ì •ì_ë©´ì—­í•™ì _ìš”ì¸","ë¶ˆì„_ì›ì¸_-_ì •ì_ìš´ë™ì„±","ë¶ˆì„_ì›ì¸_-_ì •ì_í˜•íƒœ"
    ]:
        if c in train.columns:
            bool_cols.append(c)

    # 6) ë°°ì•„_ìƒì„±_ì£¼ìš”_ì´ìœ  (ë²”ì£¼í˜•)
    if "ë°°ì•„_ìƒì„±_ì£¼ìš”_ì´ìœ " in train.columns:
        cat_cols.append("ë°°ì•„_ìƒì„±_ì£¼ìš”_ì´ìœ ")

    # 7) ì‹œìˆ , ì„ì‹ , ì¶œì‚° íšŸìˆ˜ ë³€ìˆ˜ (0íšŒ~6íšŒ ì´ìƒ)
    cycle_cols = [
        "ì´_ì‹œìˆ _íšŸìˆ˜","í´ë¦¬ë‹‰_ë‚´_ì´_ì‹œìˆ _íšŸìˆ˜","IVF_ì‹œìˆ _íšŸìˆ˜","DI_ì‹œìˆ _íšŸìˆ˜",
        "ì´_ì„ì‹ _íšŸìˆ˜","IVF_ì„ì‹ _íšŸìˆ˜","DI_ì„ì‹ _íšŸìˆ˜","ì´_ì¶œì‚°_íšŸìˆ˜","IVF_ì¶œì‚°_íšŸìˆ˜","DI_ì¶œì‚°_íšŸìˆ˜"
    ]
    for c in cycle_cols:
        if c in train.columns:
            train[c] = train[c].apply(parse_cycle_count)

    # 8) ë‚œì/ì •ì ì¶œì²˜ (ë²”ì£¼í˜•)
    if "ë‚œì_ì¶œì²˜" in train.columns:
        cat_cols.append("ë‚œì_ì¶œì²˜")
    if "ì •ì_ì¶œì²˜" in train.columns:
        cat_cols.append("ì •ì_ì¶œì²˜")

    # 9) ë‚œì ê¸°ì¦ì ë‚˜ì´, ì •ì ê¸°ì¦ì ë‚˜ì´ (ìˆ«ì ë³€í™˜ ê°€ëŠ¥ ì‹œ)
    #    (ì‚¬ìš©ìê°€ í•„ìš”í•˜ë©´ parse í•¨ìˆ˜ ë³„ë„ ì •ì˜)
    #    ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ object -> float ë³€í™˜ë§Œ ì²˜ë¦¬
    for c in ["ë‚œì_ê¸°ì¦ì_ë‚˜ì´","ì •ì_ê¸°ì¦ì_ë‚˜ì´"]:
        if c in train.columns:
            train[c] = pd.to_numeric(train[c], errors="coerce")

    # 10) ë‚œì ì±„ì·¨/í•´ë™/í˜¼í•© ê²½ê³¼ì¼, ë°°ì•„ ì´ì‹/í•´ë™ ê²½ê³¼ì¼ (ìˆ«ìí˜•)
    day_cols = ["ë‚œì_ì±„ì·¨_ê²½ê³¼ì¼","ë‚œì_í•´ë™_ê²½ê³¼ì¼","ë‚œì_í˜¼í•©_ê²½ê³¼ì¼","ë°°ì•„_ì´ì‹_ê²½ê³¼ì¼","ë°°ì•„_í•´ë™_ê²½ê³¼ì¼"]
    for c in day_cols:
        if c in train.columns:
            train[c] = pd.to_numeric(train[c], errors="coerce")
            train[c].fillna(-1, inplace=True)

    # (B-1) bool_cols â†’ 0/1 ë³€í™˜
    for c in bool_cols:
        if c in train.columns:
            train[c] = train[c].apply(parse_bool)

    # (B-2) cat_cols â†’ Label Encoding (ê²°ì¸¡ -> 'unknown' í¬í•¨)
    #      í•™ìŠµ ì‹œì ì— 'unknown'ì„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€í•´ë‘ë©´ ì¶”ë¡  ì‹œ unseen labelë„ ì²˜ë¦¬ ê°€ëŠ¥
    for c in cat_cols:
        if c in train.columns:
            train[c].fillna("unknown", inplace=True)
            unique_vals = train[c].astype(str).unique().tolist()
            if "unknown" not in unique_vals:
                unique_vals.append("unknown")
            le_cat = LabelEncoder()
            le_cat.fit(unique_vals)
            train[c] = le_cat.transform(train[c].astype(str))
            label_encoders[c] = le_cat

    # (B-3) ê²°ì¸¡ì¹˜ ì²˜ë¦¬: ì¤‘ìš”í•œ ë³€ìˆ˜ ì¤‘ê°„ê°’, ë‚˜ë¨¸ì§€ëŠ” -1
    imp_cols = ["ì‹œìˆ _ë‹¹ì‹œ_ë‚˜ì´","ë‚œì_ê¸°ì¦ì_ë‚˜ì´","ì •ì_ê¸°ì¦ì_ë‚˜ì´","ì„ì‹ _ì‹œë„_ë˜ëŠ”_ë§ˆì§€ë§‰_ì„ì‹ _ê²½ê³¼_ì—°ìˆ˜"]
    for c in imp_cols:
        if c in train.columns:
            median_val = train[c].median()
            train[c].fillna(median_val, inplace=True)

    train.fillna(-1, inplace=True)

    # (B-4) ë‚˜ì´ ë²”ìœ„ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° (ì˜ˆ: 15~60ì„¸)
    if "ì‹œìˆ _ë‹¹ì‹œ_ë‚˜ì´" in train.columns:
        train = train[(train["ì‹œìˆ _ë‹¹ì‹œ_ë‚˜ì´"] >= 15) & (train["ì‹œìˆ _ë‹¹ì‹œ_ë‚˜ì´"] <= 60)]

    # (B-5) ë°°ì•„ ê´€ë ¨ íŒŒìƒ ë³€ìˆ˜
    if "ì´_ìƒì„±_ë°°ì•„_ìˆ˜" in train.columns and "ì´ì‹ëœ_ë°°ì•„_ìˆ˜" in train.columns:
        train["ë°°ì•„_ì´ì‹_ë¹„ìœ¨"] = np.where(
            train["ì´_ìƒì„±_ë°°ì•„_ìˆ˜"] == 0,
            0,
            train["ì´ì‹ëœ_ë°°ì•„_ìˆ˜"] / train["ì´_ìƒì„±_ë°°ì•„_ìˆ˜"]
        )

    # ì˜ˆì‹œë¡œ ëª‡ ê°€ì§€ ë”:
    if "ì‹œìˆ _ë‹¹ì‹œ_ë‚˜ì´" in train.columns and "ì´ì‹ëœ_ë°°ì•„_ìˆ˜" in train.columns:
        train["ë‚˜ì´xë°°ì•„ìˆ˜"] = train["ì‹œìˆ _ë‹¹ì‹œ_ë‚˜ì´"] * train["ì´ì‹ëœ_ë°°ì•„_ìˆ˜"]
    if "ì‹œìˆ _ë‹¹ì‹œ_ë‚˜ì´" in train.columns and "ì´_ìƒì„±_ë°°ì•„_ìˆ˜" in train.columns:
        train["ë‚˜ì´xì´ë°°ì•„ìˆ˜"] = train["ì‹œìˆ _ë‹¹ì‹œ_ë‚˜ì´"] * train["ì´_ìƒì„±_ë°°ì•„_ìˆ˜"]

    # (C) X, y ë¶„ë¦¬
    X = train.drop(columns=[target_col])
    y = train[target_col]

    # (C-1) Feature Selection (LightGBM ê¸°ë°˜)
    selector_model = lgb.LGBMClassifier(
        n_estimators=300,
        learning_rate=0.05,
        random_state=42,
        boosting_type="gbdt"
    )
    selector_model.fit(X, y)

    selector = SelectFromModel(selector_model, threshold="median", prefit=True)
    selected_features = X.columns[selector.get_support()]
    X_selected = X[selected_features].copy()

    # ë¬´í•œëŒ€/ê²°ì¸¡ ì œê±°
    X_selected.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_selected.dropna(inplace=True)
    y = y.loc[X_selected.index]

    # íƒ€ì… ë³€í™˜ (float32 ë“±)
    X_selected = X_selected.astype(np.float32)
    y = y.astype(int)

    # (C-2) SMOTE (ì˜¤ë²„ìƒ˜í”Œë§)
    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X_selected, y)

    # (C-3) StandardScaler
    scaler = StandardScaler()
    X_resampled = scaler.fit_transform(X_resampled)

    ###################################################
    # 2) Optunaë¡œ LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
    ###################################################
    # ì´ˆê¸° íŒŒë¼ë¯¸í„° (Â±20% íƒìƒ‰)
    initial_params = {
        "learning_rate": 0.01,
        "n_estimators": 1000,
        "num_leaves": 31,
        "max_depth": 7,
        "min_child_samples": 20,
        "min_split_gain": 0.0,
        "reg_alpha": 0.0,
        "reg_lambda": 0.0,
        "colsample_bytree": 0.8,
        "subsample": 0.8,
        "bagging_freq": 5,
        "max_bin": 255,
        "extra_trees": False,
        "device": "gpu",
        "gpu_use_dp" : True
    }

    # í‰ê°€ í•¨ìˆ˜ (10-Fold CV â†’ AUC)
    def evaluate_params(params):
        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
        auc_scores = []
        for train_idx, val_idx in cv.split(X_resampled, y_resampled):
            X_train, X_val = X_resampled[train_idx], X_resampled[val_idx]
            y_train, y_val = y_resampled[train_idx], y_resampled[val_idx]
            model = lgb.LGBMClassifier(
                boosting_type="gbdt",
                objective="binary",
                metric="auc",
                random_state=42,
                **params
            )
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                eval_metric="auc",
                callbacks=[
                    lgb.early_stopping(stopping_rounds=50),
                    lgb.log_evaluation(period=100)
                ]
            )
            preds = model.predict_proba(X_val)[:, 1]
            auc_scores.append(roc_auc_score(y_val, preds))
        return np.mean(auc_scores)

    # Â±20% ë²”ìœ„
    def apply_variation(value, ratio=0.2):
        lower = value * (1 - ratio)
        upper = value * (1 + ratio)
        return lower, upper

    # Optuna objective
    best_auc_global = 0.0

    def objective(trial):
        global best_auc_global
        params = {}

        # 1) learning_rate
        lr_lower, lr_upper = apply_variation(initial_params["learning_rate"])
        params["learning_rate"] = trial.suggest_float("learning_rate", max(0.0001, lr_lower), lr_upper, log=True)

        # 2) n_estimators
        n_est_lower = max(100, int(initial_params["n_estimators"] * 0.8))
        n_est_upper = int(initial_params["n_estimators"] * 1.2)
        params["n_estimators"] = trial.suggest_int("n_estimators", n_est_lower, n_est_upper)

        # 3) num_leaves
        leaves_lower = max(10, int(initial_params["num_leaves"] * 0.8))
        leaves_upper = int(initial_params["num_leaves"] * 1.2)
        params["num_leaves"] = trial.suggest_int("num_leaves", leaves_lower, leaves_upper)

        # 4) max_depth
        depth_lower = max(3, int(initial_params["max_depth"] * 0.8))
        depth_upper = int(initial_params["max_depth"] * 1.2)
        params["max_depth"] = trial.suggest_int("max_depth", depth_lower, depth_upper)

        # 5) min_child_samples
        child_lower = max(5, int(initial_params["min_child_samples"] * 0.8))
        child_upper = int(initial_params["min_child_samples"] * 1.2)
        params["min_child_samples"] = trial.suggest_int("min_child_samples", child_lower, child_upper)

        # 6) min_split_gain
        gain_lower, gain_upper = apply_variation(initial_params["min_split_gain"])
        params["min_split_gain"] = trial.suggest_float("min_split_gain", gain_lower, gain_upper)

        # 7) reg_alpha
        alpha_lower, alpha_upper = apply_variation(initial_params["reg_alpha"])
        params["reg_alpha"] = trial.suggest_float("reg_alpha", alpha_lower, alpha_upper)

        # 8) reg_lambda
        lambda_lower, lambda_upper = apply_variation(initial_params["reg_lambda"])
        params["reg_lambda"] = trial.suggest_float("reg_lambda", lambda_lower, lambda_upper)

        # 9) colsample_bytree
        col_lower, col_upper = apply_variation(initial_params["colsample_bytree"])
        params["colsample_bytree"] = trial.suggest_float("colsample_bytree", max(0.1, col_lower), min(1.0, col_upper))

        # 10) subsample
        subs_lower, subs_upper = apply_variation(initial_params["subsample"])
        params["subsample"] = trial.suggest_float("subsample", max(0.1, subs_lower), min(1.0, subs_upper))

        # 11) bagging_freq
        freq_lower = max(1, int(initial_params["bagging_freq"] - 2))
        freq_upper = initial_params["bagging_freq"] + 2
        params["bagging_freq"] = trial.suggest_int("bagging_freq", freq_lower, freq_upper)

        # 12) max_bin
        bin_lower = max(50, int(initial_params["max_bin"] * 0.8))
        bin_upper = int(initial_params["max_bin"] * 1.2)
        params["max_bin"] = trial.suggest_int("max_bin", bin_lower, bin_upper)

        # 13) extra_trees
        params["extra_trees"] = trial.suggest_categorical("extra_trees", [False, True])

        # í‰ê°€
        score = evaluate_params(params)

        # ìµœê³  ì„±ëŠ¥ ê°±ì‹  ì‹œ JSON íŒŒì¼ì— ì €ì¥
        if score > best_auc_global:
            best_auc_global = score
            with open("best_lgb_params.json", "w") as f:
                json.dump(params, f, indent=4)
            print(f"\nğŸ¯ ìƒˆë¡œìš´ ìµœê³  AUC: {score:.6f}")
            print(f"ğŸ” ìµœì  íŒŒë¼ë¯¸í„°: {params}")

        return score

    # Optuna ì‹¤í–‰
    from optuna.samplers import TPESampler
    from optuna.pruners import HyperbandPruner
    sampler = TPESampler(multivariate=True)
    pruner = HyperbandPruner()
    study = optuna.create_study(direction="maximize", sampler=sampler, pruner=pruner)
    study.optimize(objective, n_trials=1000)

    print("âœ… Best trial:", study.best_trial)
    best_params = study.best_trial.params
    best_params.update({
        "boosting_type": "gbdt",
        "objective": "binary",
        "metric": "auc",
        "random_state": 42
    })
    print("\nâœ… ìµœì  íŒŒë¼ë¯¸í„°:")
    print(json.dumps(best_params, indent=4))

    ###################################################
    # 3) ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
    ###################################################
    best_lgb_model = lgb.LGBMClassifier(**best_params)
    best_lgb_model.fit(
        X_resampled, y_resampled,
        callbacks=[lgb.log_evaluation(-1)]
    )

    # í•™ìŠµ ë°ì´í„° ì„±ëŠ¥ í™•ì¸
    train_preds = best_lgb_model.predict_proba(X_resampled)[:, 1]
    train_pred_class = (train_preds >= 0.5).astype(int)
    train_auc = roc_auc_score(y_resampled, train_preds)
    train_acc = accuracy_score(y_resampled, train_pred_class)
    print("\nğŸ”¥ Final Model Evaluation (Train Data)")
    print(f"ğŸ”¥ Train Accuracy: {train_acc:.4f}")
    print(f"ğŸ”¥ Train ROC-AUC: {train_auc:.4f}")

    # ê°ì²´ ì €ì¥ (ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, í”¼ì²˜ ëª©ë¡, ë¼ë²¨ ì¸ì½”ë”)
    joblib.dump(best_lgb_model, "best_lgb_model.pkl")
    joblib.dump(scaler, "scaler.pkl")
    joblib.dump(list(X_selected.columns), "train_features.pkl")
    joblib.dump(label_encoders, "label_encoders.pkl")  # ì¶”ë¡  ì‹œ ì‚¬ìš©
    print("âœ… ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, í”¼ì²˜ ëª©ë¡, ë¼ë²¨ ì¸ì½”ë” ì‚¬ì „ ì €ì¥ ì™„ë£Œ!")
